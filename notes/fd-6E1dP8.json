{"_id":"note:fd-6E1dP8","title":"图像对齐","content":"# 图像对齐\n\n## SIFT算法\n\nSIFT算法的实质可以归为在不同尺度空间上查找特征点（关键点）的问题。\n\n### 检测步骤\n\n1. 提取关键点； \n\n2. 对关键点附加详细的信息（局部特征）也就是所谓的描述器；\n\n3. 通过两方特征点（附带上特征向量的关键点）的两两比较找出相互匹配的若干对特征点，也就建立了景物间的对应关系。\n\n### BF\n\nBFmatcher（Brute-Force Matching）暴力匹配，应用BFMatcher.knnMatch( )函数来进行核心的匹配，knnMatch（k-nearest neighbor classification）k近邻分类算法。\nkNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\nkNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。\n经检验 BFmatcher在做匹配时会耗费大量的时间。\n\n```python\nimport cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nEXIT_KEY = 27\n\n\ndef sift(image_path1, image_path2):\n    # Read image data\n    image1 = cv.imread(image_path1)\n    image2 = cv.imread(image_path2)\n\n    # Resize\n    image1 = cv.resize(image1, (512, 512))\n    image2 = cv.resize(image2, (512, 512))\n\n    cv.imshow(\"Original Image 1\", image1)\n    cv.imshow(\"Original Image 2\", image2)\n\n    # Get SIFT Key points\n    sift_dsc = cv.SIFT_create(50, 5, 0.01, 80)\n    keypoint1, features1 = sift_dsc.detectAndCompute(image1, None)\n    keypoint2, features2 = sift_dsc.detectAndCompute(image2, None)\n\n    # Draw key points on\n    image_with_kp_1, image_with_kp_2 = None, None\n    # @param:image\n    # @param:keypoints\n    # @param:outImage\n    # @param:color\n    # @param:flags\n    image_with_kp_1 = cv.drawKeypoints(image1, keypoint1, outImage=None,\n                                       flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    image_with_kp_2 = cv.drawKeypoints(image2, keypoint2, outImage=None,\n                                       flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n    # Show key points\n    cv.imshow(\"Image 1 with keypoints\", image_with_kp_1)\n    cv.imshow(\"Image 2 with keypoints\", image_with_kp_2)\n\n    # Merge 2 images horizontally\n    h_merge = np.hstack((image1, image2))\n    cv.imshow(\"Horizontal merge\", h_merge)\n\n    # BF Matcher\n    bf = cv.BFMatcher()\n    matches = bf.knnMatch(features1, features2, k=2)\n    bf_knn_match_image = cv.drawMatchesKnn(img1=image1,\n                                           keypoints1=keypoint1,\n                                           img2=image2,\n                                           keypoints2=keypoint2,\n                                           matches1to2=matches,\n                                           outImg=None,\n                                           flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    cv.imshow(\"Brute-Force Matching\", bf_knn_match_image)\n\n    while cv.waitKey(0) != EXIT_KEY:\n        exit(0)\n\n```\n\n\n### Flann\n\nFLANN(Fast_Library_for_Approximate_Nearest_Neighbors)快速最近邻搜索包，它是一个对大数据集和高维特征进行最近邻搜索的算法的集合,而且这些算法都已经被优化过了。在面对大数据集时它的效果要好于 BFMatcher。\n经验证，FLANN比其他的最近邻搜索软件快10倍。使用 FLANN 匹配,我们需要传入两个字典作为参数。这两个用来确定要使用的算法和其他相关参数等。\n第一个是 IndexParams。\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) 。\n这里使用的是KTreeIndex配置索引，指定待处理核密度树的数量（理想的数量在1-16）。\n第二个字典是SearchParams。\nsearch_params = dict(checks=100)用它来指定递归遍历的次数。值越高结果越准确，但是消耗的时间也越多。实际上，匹配效果很大程度上取决于输入。\n5kd-trees和50checks总能取得合理精度，而且短时间完成。在之下的代码中，丢弃任何距离大于0.7的值，则可以避免几乎90%的错误匹配，但是好的匹配结果也会很少。\n\n\n\n```python\nimport cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nEXIT_KEY = 27\n\n\ndef sift(image_path1, image_path2):\n    # Read image data\n    image1 = cv.imread(image_path1)\n    image2 = cv.imread(image_path2)\n\n    # Resize\n    image1 = cv.resize(image1, (512, 512))\n    image2 = cv.resize(image2, (512, 512))\n\n    cv.imshow(\"Original Image 1\", image1)\n    cv.imshow(\"Original Image 2\", image2)\n\n    # Get SIFT Key points\n    sift_dsc = cv.SIFT_create(50, 5, 0.01, 80)\n    keypoint1, features1 = sift_dsc.detectAndCompute(image1, None)\n    keypoint2, features2 = sift_dsc.detectAndCompute(image2, None)\n\n    # Draw key points on\n    image_with_kp_1, image_with_kp_2 = None, None\n    # @param:image\n    # @param:keypoints\n    # @param:outImage\n    # @param:color\n    # @param:flags\n    image_with_kp_1 = cv.drawKeypoints(image1, keypoint1, outImage=None,\n                                       flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    image_with_kp_2 = cv.drawKeypoints(image2, keypoint2, outImage=None,\n                                       flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n    # Show key points\n    cv.imshow(\"Image 1 with keypoints\", image_with_kp_1)\n    cv.imshow(\"Image 2 with keypoints\", image_with_kp_2)\n\n    # Merge 2 images horizontally\n    h_merge = np.hstack((image1, image2))\n    cv.imshow(\"Horizontal merge\", h_merge)\n\n    # FlannBasedMatcher\n    flann = cv.FlannBasedMatcher()\n    matches = flann.knnMatch(features1, features2, k=2)\n    flann_knn_match_image = cv.drawMatchesKnn(img1=image1,\n                                              keypoints1=keypoint1,\n                                              img2=image2,\n                                              keypoints2=keypoint2,\n                                              matches1to2=matches,\n                                              outImg=None,\n                                              flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n    cv.imshow(\"Fast Library for Approximate Nearest Neighbors Matching\", flann_knn_match_image)\n\n    while cv.waitKey(0) != EXIT_KEY:\n        exit(0)\n\n```\n\n#### 效果\n\n![效果](/home/ling/BoostNote/images/sift.png)","tags":[],"folderPathname":"/图像处理","data":{},"createdAt":"2020-12-06T12:39:39.204Z","updatedAt":"2020-12-06T14:13:38.069Z","trashed":false,"_rev":"4sKkkSf4e"}