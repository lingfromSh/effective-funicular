{"_id":"note:UAowIaj7M","title":"技术简史","content":"# 技术简史\n\n## One-Hot\n\n把整个样本库的所有词去重后整成一个巨大的词典，假设里面有2673个不同的词吧，就用1*2673的矩阵来表示，每一维只代表一个词，绝不重复。\n\n缺点: 信息太稀疏\n\n## Word2Vec\n\nWord2Vec同样要完成词的向量化，和One-Hot最大的不同是短很多，上面One-Hot表示一个词需要1*2673，而在Word2Vec中维度可能只需40维或60维，显然，Word2Vec要稠密多了。\n\n### Word Embedding\n\n1. One-Hot\n2. Skip-gram\n3. Word2Vec\n\n## Bert (PTM)\n\n预训练模型\n\n","tags":[],"folderPathname":"/NLP","data":{},"createdAt":"2020-12-23T07:11:36.210Z","updatedAt":"2020-12-23T07:15:28.159Z","trashed":false,"_rev":"32n0d2BNk"}