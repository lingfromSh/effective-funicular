{"_id":"note:rBvsBmv4W","title":"智能图像分割","content":"# 智能图像分割\n\n## Unet\n\n分割细胞\n\n### Unet定义\n\n#### unet_parts.py\n\n```python\n\"\"\" Parts of the U-Net model \"\"\"\n\"\"\"https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n```\n\n#### unet_model.py\n\n```python\nimport torch.nn.functional as F\n\nfrom .unet_parts import *\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 512)\n        self.up1 = Up(1024, 256, bilinear)\n        self.up2 = Up(512, 128, bilinear)\n        self.up3 = Up(256, 64, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n```\n\n#### train.py\n\n```python\nfrom model.unet_model import UNet\nfrom utils.dataset import ISBI_Loader\nfrom torch import optim\nimport torch.nn as nn\nimport torch\n\ndef train_net(net, device, data_path, epochs=40, batch_size=1, lr=0.00001):\n    # 加载训练集\n    isbi_dataset = ISBI_Loader(data_path)\n    train_loader = torch.utils.data.DataLoader(dataset=isbi_dataset,\n                                               batch_size=batch_size, \n                                               shuffle=True)\n    # 定义RMSprop算法\n    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n    # 定义Loss算法\n    criterion = nn.BCEWithLogitsLoss()\n    # best_loss统计，初始化为正无穷\n    best_loss = float('inf')\n    # 训练epochs次\n    for epoch in range(epochs):\n        # 训练模式\n        net.train()\n        # 按照batch_size开始训练\n        for image, label in train_loader:\n            optimizer.zero_grad()\n            # 将数据拷贝到device中\n            image = image.to(device=device, dtype=torch.float32)\n            label = label.to(device=device, dtype=torch.float32)\n            # 使用网络参数，输出预测结果\n            pred = net(image)\n            # 计算loss\n            loss = criterion(pred, label)\n            print('Loss/train', loss.item())\n            # 保存loss值最小的网络参数\n            if loss < best_loss:\n                best_loss = loss\n                torch.save(net.state_dict(), 'best_model.pth')\n            # 更新参数\n            loss.backward()\n            optimizer.step()\n\n```\n","tags":[],"folderPathname":"/","data":{},"createdAt":"2020-12-07T10:12:44.184Z","updatedAt":"2020-12-07T10:15:21.215Z","trashed":true,"_rev":"AQ_2CefRP"}